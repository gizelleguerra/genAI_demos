{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMi5iU2cIJuDH5lEcVSZZ7u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gizelleguerra/genAI_demos/blob/main/Resume_GPT_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Version of GPT Resume Seach Tool"
      ],
      "metadata": {
        "id": "z8gRM2Q6lmLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs"
      ],
      "metadata": {
        "id": "jv8MZ4iillIG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX0_ze2DI1-C",
        "outputId": "bcdff04b-ad53-4607-c168-9f61abc8be28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.178-py3-none-any.whl (892 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m892.2/892.2 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-0.3.25-py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.6/86.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.5.3)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hnswlib>=0.7 (from chromadb)\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb)\n",
            "  Downloading clickhouse_connect-0.5.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.6/922.6 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.1)\n",
            "Collecting fastapi>=0.85.1 (from chromadb)\n",
            "  Downloading fastapi-0.95.2-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.14.1-cp310-cp310-manylinux_2_27_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.3.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.15)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.7.1)\n",
            "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb)\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect>=0.5.7->chromadb)\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.85.1->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (414 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.1/414.1 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (3.6.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (1.3.0)\n",
            "Building wheels for collected packages: hnswlib\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2119845 sha256=adbd01a4f547a1d72a68dead855c663e4bd79482949d99132e994ad3a36b96c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "Successfully built hnswlib\n",
            "Installing collected packages: tokenizers, monotonic, faiss-cpu, zstandard, websockets, uvloop, requests, python-dotenv, overrides, mypy-extensions, multidict, marshmallow, lz4, humanfriendly, httptools, hnswlib, h11, frozenlist, backoff, async-timeout, yarl, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, openapi-schema-pydantic, marshmallow-enum, coloredlogs, clickhouse-connect, aiosignal, onnxruntime, fastapi, dataclasses-json, aiohttp, openai, langchain, chromadb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 backoff-2.2.1 chromadb-0.3.25 clickhouse-connect-0.5.24 coloredlogs-15.0.1 dataclasses-json-0.5.7 faiss-cpu-1.7.4 fastapi-0.95.2 frozenlist-1.3.3 h11-0.14.0 hnswlib-0.7.0 httptools-0.5.0 humanfriendly-10.0 langchain-0.0.178 lz4-4.3.2 marshmallow-3.19.0 marshmallow-enum-1.5.1 monotonic-1.6 multidict-6.0.4 mypy-extensions-1.0.0 onnxruntime-1.14.1 openai-0.27.7 openapi-schema-pydantic-1.2.4 overrides-7.3.1 posthog-3.0.1 python-dotenv-1.0.0 requests-2.31.0 starlette-0.27.0 tiktoken-0.4.0 tokenizers-0.13.3 typing-inspect-0.8.0 uvicorn-0.22.0 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3 yarl-1.9.2 zstandard-0.21.0\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install --upgrade langchain faiss-cpu chromadb openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2Ed8-fkbl7a",
        "outputId": "ca2db38a-ec19-4c30-8372-ddf19a042b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.9.0-py3-none-any.whl (249 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.5/249.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Google Drive Mount"
      ],
      "metadata": {
        "id": "yr-PcJxkXh3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ9hIEVVkwXG",
        "outputId": "a6b69dbc-327a-413b-917e-cc8a8ce893e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "CqfMpSyUgleW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse, json, time, datetime, openai\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "uDy_bl8-k09l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_open_ai_key(env_path=None):\n",
        "  #import json, os\n",
        "  #from pathlib import Path\n",
        "  try:\n",
        "    with open(env_path, \"r\") as f:\n",
        "        env_vars = json.load(f)\n",
        "    os.environ[\"OPENAI_API_KEY\"] = env_vars[\"OPENAI_API_KEY\"]\n",
        "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "    #os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
        "    openai.Model.list() #test a random command on the openai API\n",
        "    return True\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "  return False"
      ],
      "metadata": {
        "id": "oCNd95EKlLSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "n32VQn9nlxtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup API key\n",
        "openai_env_path, openai.api_key = None, None\n",
        "cwd = Path.cwd()\n",
        "# resume_path = cwd / \"Resumes\"\n",
        "# resume_path.mkdir(exist_ok=True)\n",
        "\n",
        "openai_env_path = cwd/ \"drive/MyDrive/Colab Notebooks/openai.env\"\n",
        "set_open_ai_key(openai_env_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9zeIWbTlOUz",
        "outputId": "d92e9518-6619-43a2-fb8d-c8df13a5d22d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Parse Resume Books"
      ],
      "metadata": {
        "id": "CFVKYyYVrRDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cwd = Path.cwd()\n",
        "#output_path = cwd / \"drive/MyDrive/Colab Notebooks/Output\""
      ],
      "metadata": {
        "id": "tM9e27fUmf28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up document loader for pdf resume books\n",
        "embeddings = OpenAIEmbeddings()\n",
        "resume_path1 = \"/content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf\"\n",
        "resume_path2 = \"/content/drive/MyDrive/Colab Notebooks/resume_books/GDI2022ResumeBook.pdf\""
      ],
      "metadata": {
        "id": "oGC0aYW3goGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_resumes(path, skip_pages):\n",
        "  loader = PyPDFLoader(path)\n",
        "  pages = loader.load_and_split()\n",
        "  pages_clean = pages[skip_pages:]\n",
        "  return pages_clean"
      ],
      "metadata": {
        "id": "Iq6yjwSrmQtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# actual resumes start on page 2 of this pdf compilation\n",
        "r1 = load_resumes(resume_path1, 2)"
      ],
      "metadata": {
        "id": "0hRYBwDHwYut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# actual resumes start on page 1 of this pdf compilation\n",
        "r2 = load_resumes(resume_path2, 1)"
      ],
      "metadata": {
        "id": "jRf5565wwlwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r1[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe_VpfOCw_Ix",
        "outputId": "08cba013-07dc-44ca-d4fc-dcc54a8dd96b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='YIN FU(206) 889-7382 ■fuyin1999@gmail.com■linkedin.com/in/yinfu1■https://github.com/fuyin19EDUCATIONNEW YORK UNIVERSITYNew York, NYThe Courant Institute of Mathematical SciencesM.S. in Mathematics in Finance(Sep. 2021 - Dec. 2022)●Coursework:Stochasticcalculus,derivativepricing,quantitativeportfoliotheory,riskmanagement,financialdata science and machine learning, time series analysis, interest rate modelingUNIVERSITY OF WASHINGTONSeattle, WABS in Mathematics(Sep. 2017 – Jun. 2021)●Coursework:Probability, linear algebra, numericalanalysis, statistics, ODEs and PDEs, measure theory●Honors:Magna Cum Laude (Top 3.5%), Dean’s ListEXPERIENCECHINA CONSTRUCTION BANK, NEW YORK BRANCHNew York, NYQuantitative Risk Analyst(Jun. 2022 – Aug. 2022)●Builtacountryriskpredictorleveraginglinearmodels,boosting,randomforestbasedonS&Pdataofeconomics and political factors, and achieved 87.2% in-sample and 78.9% out-of-sample accuracy●Drafted country risk report for the US collaboratively by analyzing macro risk factors and ML predictions●Implementedthestock-flowcyclemodelfortheUSrealestatemarket,andcalibratedparameterstothemarket data from 1980 to 2022; tuned hyperparameters for model interpretability and performanceWASHINGTON EXPERIMENTAL MATHEMATICAL LAB - WXMLSeattle, WAResearch Assistant(Apr. 2020 – Dec. 2020)●Derived mathematical properties of number operators and Hamiltonians in bosonic quantum field theory●Proved non-uniqueness of field configuration, given the same observation in Minkowski particle content●Explained theoretical behavior of number operators’ in real-world termsPROJECTSNEW YORK UNIVERSITYNew York, NYSimulation of Backward SDEs and Applications to Nonlinear PDEs in Finance(Python)●Implemented deep BSDE and generalized LSMC method for nonlinear PDEs based on ML algorithms●Option Pricing:Priced exotic options by simulationof BSDEs and derived dynamic hedging strategies●OptimalExecution:LeveragedLSMCtosolvetheHJB-PDEsinequitymarketimpactmodelspresentedbyCartea et al. (2015) for optimal inventory processes, and analyzed convergence, numerical stability, etcImplied and Local Volatility Calibration(Python)●CalibratedSVIparameterizationwithSPXoptionsdatatoacontinuousimpliedvolatilitysurface,andcomputed local volatility surfaceBacktesting and Statistical Arbitrage(Python)●ResearchedandpresentedtheCNN+TransformermodelinDeepLearningStatisticalArbitrage(2020)andanalyzed the out-of-sample performance for 550 largest US stocks with different risk factors.●ImplementedandbacktestedtheAdaptedP&QstrategypresentedbyFongandTai(2009)forS&P500stocks, calculated performance metrics (ROI, Sharp ratio), and analyzed the impact of market frictions.Financial Data Science(Python)●Index Tracking:Built a dynamic index tracking strategyfor S&P 500 leveraging Kalman filter●ICA:PerformedpICAonReutersnewstoidentifythemostrelatedarticlestospecifictopicssuchasearnings, rates, and CPI; analyzed and compared the performance to PCA-based LSA.UNIVERSITY OF WASHINGTONSeattle, WAIntroduction to Numerical Methods for Solving Large and Sparse Linear Systems(MATLAB)●Elaborated Krylov subspace methods and implemented conjugate gradient method in MATLAB●Researched numerical limitations of current best sparse linear system solver by Peng and Vampala (2020)COMPUTATIONAL SKILLS/OTHERProgramming Languages:Python, Java, MATLAB, MathematicaLanguages:English (fluent), Mandarin (native), Japanese(intermediate)', metadata={'source': '/content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf', 'page': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r1[0].metadata[\"name\"] = \"YIN FU\""
      ],
      "metadata": {
        "id": "BV9h2YP1I-P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r1[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s36glM89JtNd",
        "outputId": "2f56cd34-a4ad-47ac-9b98-d711058d2084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='ARJUN KALSI\\narjun.kalsi@nyu.edu\\n■\\nlinkedin.com/in/arjunkalsi1\\n■\\ngithub.com/arjunkalsi\\n. \\nEDUCA TION\\nNEW YORK UNIVERSITY\\nNew\\nYork, NY\\nThe Courant Institute of Mathematical Sciences\\nMS in Mathematics in Finance\\n(expected-Dec 2022) \\n●\\nCoursework:\\nMonte\\nCarlo\\nmethods,\\nBrownian\\nmotion,\\nsupervised/unsupervised\\nlearning,\\nfeature map regression, cross-validation, neural networks, data cleaning and web-scraping\\nUNIVERSITY OF WARWICK\\nCoventry, UK\\nBS Mathematics, Operational Research, Statistics, Economics\\n(Sep 2018-Jun 2021)\\n●\\nCoursework:\\nmathematical analysis, linear algebra,\\nprobability, Bayesian statistics and decision\\ntheory, MLE,  options pricing, linear statistical modeling, stochastic processes\\nEXPERIENCE\\nSOFR ACADEMY -\\nQuantitative Analyst Intern\\n(Dec 2021-June\\n2022)\\nNew York, NY\\n●\\nConstructed a publication handbook for the firm’s new Across-the-Curve Credit Spread Index \\n(AXI) tool, explaining all automated code related to the data retrieval process via AWS, and\\npresented this to prospective clients\\n●\\nApplied AXI values to 1 year of historical JPY data using Python for an ongoing non-USD AXI\\nfeasibility study aimed at increasing the versatility of the index\\n●\\nCollaborated with Japanese colleagues to research JPY transaction data sources for short-term\\nmoney market instruments data, as well as long-term bond transactions data\\nH2 VENTURES -\\nVenture Capital Intern\\n(Jul 2020-Aug\\n2020)\\nLondon, UK\\n●\\nIdentified 10 promising start-ups in the healthtech industry and built a grading scale function \\nusing Python in order to rank them, allocating points based on risk, management, business\\nstrategy, and exit opportunities\\n●\\nEvaluated 4 start-ups in the firm’s portfolio using the venture capital method as well as DCF\\nanalysis, and pitched investment strategies to peers based on these results\\nPROJECTS\\nNEW YORK UNIVERSITY\\n-\\nTrading Energy Derivatives Project\\n(Python)\\n●\\nUsed a rolling regression model on USD rates, inflation rates, and storage in order to develop a \\ncarry-based strategy for WTI futures\\n●\\nLeveraged Python modules such as SciPy, NumPy, and Pandas to interpolate storage data, as well as\\noptimize the rolling regression window over the last 10 years of data\\n●\\nUtilized a Middle Eastern war sentiment index to implement a threshold signal to halt trading which\\nincreased the annualized Sharpe Ratio from 0.42 to 0.61\\nUNIVERSITY OF WARWICK\\n-\\nAirlines Trading Study with\\nNLP (Python)\\n●\\nCleaned and analyzed 3 months of historical time-series data to backtest a pairs algorithm on a model\\nportfolio focussed on American Airlines and United Airlines\\n●\\nUsed NLP techniques on a US Airline Sentiment Tweets dataset to expand and contract trade volume\\n●\\nGenerated returns of 13% and visualized a distinct relationship in stock performance between two\\nfirms in the same industry\\nCOMPUTER SKILLS/OTHER\\nProgramming Languages:\\nPython, SQL, Java, R, MATLAB\\nLanguages:\\nEnglish (native), French (basic)\\nInterests:\\nMusic Producer on Spotify and Apple Music\\nwith over 20,000 monthly listeners', metadata={'source': '/content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf', 'page': 3})"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r1[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJcmjT84JLBw",
        "outputId": "0b165523-67fd-411f-8277-3c63d24e531e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='YIN FU(206) 889-7382 ■fuyin1999@gmail.com■linkedin.com/in/yinfu1■https://github.com/fuyin19EDUCATIONNEW YORK UNIVERSITYNew York, NYThe Courant Institute of Mathematical SciencesM.S. in Mathematics in Finance(Sep. 2021 - Dec. 2022)●Coursework:Stochasticcalculus,derivativepricing,quantitativeportfoliotheory,riskmanagement,financialdata science and machine learning, time series analysis, interest rate modelingUNIVERSITY OF WASHINGTONSeattle, WABS in Mathematics(Sep. 2017 – Jun. 2021)●Coursework:Probability, linear algebra, numericalanalysis, statistics, ODEs and PDEs, measure theory●Honors:Magna Cum Laude (Top 3.5%), Dean’s ListEXPERIENCECHINA CONSTRUCTION BANK, NEW YORK BRANCHNew York, NYQuantitative Risk Analyst(Jun. 2022 – Aug. 2022)●Builtacountryriskpredictorleveraginglinearmodels,boosting,randomforestbasedonS&Pdataofeconomics and political factors, and achieved 87.2% in-sample and 78.9% out-of-sample accuracy●Drafted country risk report for the US collaboratively by analyzing macro risk factors and ML predictions●Implementedthestock-flowcyclemodelfortheUSrealestatemarket,andcalibratedparameterstothemarket data from 1980 to 2022; tuned hyperparameters for model interpretability and performanceWASHINGTON EXPERIMENTAL MATHEMATICAL LAB - WXMLSeattle, WAResearch Assistant(Apr. 2020 – Dec. 2020)●Derived mathematical properties of number operators and Hamiltonians in bosonic quantum field theory●Proved non-uniqueness of field configuration, given the same observation in Minkowski particle content●Explained theoretical behavior of number operators’ in real-world termsPROJECTSNEW YORK UNIVERSITYNew York, NYSimulation of Backward SDEs and Applications to Nonlinear PDEs in Finance(Python)●Implemented deep BSDE and generalized LSMC method for nonlinear PDEs based on ML algorithms●Option Pricing:Priced exotic options by simulationof BSDEs and derived dynamic hedging strategies●OptimalExecution:LeveragedLSMCtosolvetheHJB-PDEsinequitymarketimpactmodelspresentedbyCartea et al. (2015) for optimal inventory processes, and analyzed convergence, numerical stability, etcImplied and Local Volatility Calibration(Python)●CalibratedSVIparameterizationwithSPXoptionsdatatoacontinuousimpliedvolatilitysurface,andcomputed local volatility surfaceBacktesting and Statistical Arbitrage(Python)●ResearchedandpresentedtheCNN+TransformermodelinDeepLearningStatisticalArbitrage(2020)andanalyzed the out-of-sample performance for 550 largest US stocks with different risk factors.●ImplementedandbacktestedtheAdaptedP&QstrategypresentedbyFongandTai(2009)forS&P500stocks, calculated performance metrics (ROI, Sharp ratio), and analyzed the impact of market frictions.Financial Data Science(Python)●Index Tracking:Built a dynamic index tracking strategyfor S&P 500 leveraging Kalman filter●ICA:PerformedpICAonReutersnewstoidentifythemostrelatedarticlestospecifictopicssuchasearnings, rates, and CPI; analyzed and compared the performance to PCA-based LSA.UNIVERSITY OF WASHINGTONSeattle, WAIntroduction to Numerical Methods for Solving Large and Sparse Linear Systems(MATLAB)●Elaborated Krylov subspace methods and implemented conjugate gradient method in MATLAB●Researched numerical limitations of current best sparse linear system solver by Peng and Vampala (2020)COMPUTATIONAL SKILLS/OTHERProgramming Languages:Python, Java, MATLAB, MathematicaLanguages:English (fluent), Mandarin (native), Japanese(intermediate)', metadata={'source': '/content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf', 'page': 2, 'name': 'YIN FU'})"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combine resume books\n",
        "resumes = r1+r2"
      ],
      "metadata": {
        "id": "j1uplBr4yK-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(resumes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCfhpUqSyXMG",
        "outputId": "5b10cdbe-8c96-4d68-cd8d-8c87952d01fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start with one resume book\n",
        "#loader = PyPDFLoader(resume_path1)\n",
        "#pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "C9OM776mazMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# actual resumes start on page 2 of this pdf compilation\n",
        "#resumes = pages[2:]"
      ],
      "metadata": {
        "id": "pql0d9xthd7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunk Resumes"
      ],
      "metadata": {
        "id": "vlbhc4VmXr9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the documents into chunks\n",
        "#text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "#texts = text_splitter.split_documents(resumes)"
      ],
      "metadata": {
        "id": "ZrDOxItiFpGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "0crC5LkEM9EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set small chunk size, just to test.\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap  = 0,\n",
        "    length_function = len,\n",
        ")"
      ],
      "metadata": {
        "id": "8PiGiyZuM87r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.split_documents(resumes)\n",
        "print(texts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jyo2AZKuM8vB",
        "outputId": "0458aa22-7c86-4fda-fe8a-a9a8935a1ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='YIN FU(206) 889-7382 ■fuyin1999@gmail.com■linkedin.com/in/yinfu1■https://github.com/fuyin19EDUCATIONNEW YORK UNIVERSITYNew York, NYThe Courant Institute of Mathematical SciencesM.S. in Mathematics in Finance(Sep. 2021 - Dec. 2022)●Coursework:Stochasticcalculus,derivativepricing,quantitativeportfoliotheory,riskmanagement,financialdata science and machine learning, time series analysis, interest rate modelingUNIVERSITY OF WASHINGTONSeattle, WABS in Mathematics(Sep. 2017 – Jun.' metadata={'source': '/content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf', 'page': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Vector Stores to create embeddings and preform similarity search"
      ],
      "metadata": {
        "id": "9TKLn4wRraRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to do: figure out which vector store is best to use - FAISS vs Chroma"
      ],
      "metadata": {
        "id": "SabOVPAari7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "4JSHxY4aonll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using FAISS"
      ],
      "metadata": {
        "id": "dg561mQxgxkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_index = FAISS.from_documents(texts, OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "Kz6V7RjFove3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = faiss_index.similarity_search(\"knows statistics\", k=8)"
      ],
      "metadata": {
        "id": "unIyqhefo0GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "    #print(str(doc.metadata[\"page\"]) + \":\", doc.page_content)\n",
        "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:100] + \":\", str(doc.metadata[\"source\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjMQgW5VqQAC",
        "outputId": "42064b26-6178-4125-e030-7b2267e2d2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5: Coursework:\n",
            "probability, stochastic processes, time\n",
            "series analysis, numerical analysis, statistical: /content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf\n",
            "2: 2021)●Coursework:Probability, linear algebra, numericalanalysis, statistics, ODEs and PDEs, measure : /content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf\n",
            "19: • Gained proficiency with statistical analysis software, SPSS , and statistical testing of scientifi: /content/drive/MyDrive/Colab Notebooks/resume_books/GDI2022ResumeBook.pdf\n",
            "17: previously. In addition, we recommend an intermediate course on mathematical statistics or engineeri: /content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf\n",
            "20: real-world datasets and model them in Python using techniques from statistics, quantitative finance : /content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf\n",
            "25: Relevant Coursework : Statistic s, Information Systems, Microe conomics , Macroeconomics, Calculus  : /content/drive/MyDrive/Colab Notebooks/resume_books/GDI2022ResumeBook.pdf\n",
            "3: (Sep 2018-Jun 2021)\n",
            "●\n",
            "Coursework:\n",
            "mathematical analysis, linear algebra,\n",
            "probability, Bayesian stati: /content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf\n",
            "7: cryptocurrency data to derive factormodels of highly discontinuous, noisy data sets; used Java and M: /content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNPqAj_QCYpO",
        "outputId": "75b2ece9-5fde-4967-82f0-5d1e90a42fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Relevant Coursework : Statistic s, Information Systems, Microe conomics , Macroeconomics, Calculus  \\n \\nEXPERIENCE  \\nFordham Gabelli Consulting Challenge                                                                                                                   Bronx, NY  \\nTeam Member, Report Coordinator                                                                                                             August  2022  - Present', metadata={'source': '/content/drive/MyDrive/Colab Notebooks/resume_books/GDI2022ResumeBook.pdf', 'page': 25})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Call OpenAI to use GPT to answer questions based on Resumes"
      ],
      "metadata": {
        "id": "cGlPzQUAsQJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain"
      ],
      "metadata": {
        "id": "PCqSJdoQqXOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#question = \"Who knows statistics?\"\n",
        "question = \"which resumes have statistics skills?\""
      ],
      "metadata": {
        "id": "g8oGclaEtqTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test different prompts\n",
        "context = \" Make a table of results from the documents given with columns indicating page and source.\"\n",
        "#context = \" Make a list of the page numbers from the documents given, using the 'page' metadata, remove all duplicates from the list\"\n",
        "#context = \" make a table using the document metadata table with columns: page, source\"\n",
        "#context = \" Make a list from the documents given.\"\n",
        "#context = \" return the document metadata\""
      ],
      "metadata": {
        "id": "JRu44qLy7Nq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make output in json with the following source, page, knows statistics \n",
        "#context = \" Generate a list of resulting resumes with page numbers and source file information, provide them in JSON format with the following keys: page_id, source_file\"\n",
        "#context = \" Generate a list of resulting resumes with their metadata fields, provide them in JSON format with the following keys: page_id, source_file\"\n",
        "context = \" Provide resulting resumes in JSON format with the following keys from the document metadata fields: page_id, source_file\""
      ],
      "metadata": {
        "id": "2uRsPLyvd2Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = question + context\n",
        "print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlnypD3OAeJ3",
        "outputId": "9617a9da-c3cb-4b1c-929f-fb052d12d743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "which resumes have statistics skills? Provide resulting resumes in JSON format with the following keys from the document metadata fields: page_id, source_file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "gJoL5zc4BBlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8hHlzztT66X",
        "outputId": "05503c6a-5465-4b62-c3f6-5f1550621c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output_text': ' The resumes with statistics skills are the ones on page 1, page 2, page 3, page 4, and page 5 of the resume book from /content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf.\\n\\nThe JSON format for the list of resumes with statistics skills is:\\n[\\n  {\\n    \"page_id\": 1,\\n    \"source_file\": \"/content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf\"\\n  },\\n  {\\n    \"page_id\": 2,\\n    \"source_file\": \"/content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf\"\\n  },\\n  {\\n    \"page_id\": 3,\\n    \"source_file\": \"/content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf\"\\n  },\\n  {\\n    \"page_id\": 4,\\n    \"source_file\": \"/content/drive/MyDrive/Colab Notebooks/'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#best one?\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0M8dhKB-PPp",
        "outputId": "77764471-2cb2-49b7-9322-9f4229844720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output_text': ' The resumes with statistics skills are from the following sources: \\n/content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf\\n/content/drive/MyDrive/Colab Notebooks/resume_books/GDI2022ResumeBook.pdf\\n\\nThe resumes with their metadata fields in JSON format are as follows: \\n[{\"page_id\": 0, \"source_file\": \"/content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf\"}, {\"page_id\": 1, \"source_file\": \"/content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf\"}, {\"page_id\": 2, \"source_file\": \"/content/drive/MyDrive/Colab Notebooks/resume_books/GDI2022ResumeBook.pdf\"}, {\"page_id\": 3, \"source_file\": \"/content/drive/MyDrive/Colab Notebooks/resume_books/resume_book_2022.pdf\"}, {\"page_id\": 4, \"source_'}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Chroma"
      ],
      "metadata": {
        "id": "ptuJdNRQg2pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# chain = load_qa_with_sources_chain(llm=OpenAI(), chain_type=\"stuff\")\n",
        "# chain({\"input_documents\": docs, \"question\": query}, return_source_documents=True)"
      ],
      "metadata": {
        "id": "M9VdPNvrmv_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# db = DeepLake(dataset_path=dataset_path, read_only=True, embedding_function=embeddings)\n",
        "\n",
        "# retriever = db.as_retriever()\n",
        "# retriever.search_kwargs['distance_metric'] = 'cos'\n",
        "# retriever.search_kwargs['k'] = 4\n",
        "\n",
        "# qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=False)\n",
        "\n",
        "# # What was the restaurant the group was talking about called?\n",
        "# query = input(\"Enter query:\")\n",
        "\n",
        "# # The Hungry Lobster\n",
        "# ans = qa({\"query\": query})\n",
        "\n",
        "# print(ans)"
      ],
      "metadata": {
        "id": "AXuNM4QluA7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def qa(file, query, chain_type, k):\n",
        "#     # load document\n",
        "#     loader = PyPDFLoader(file)\n",
        "#     documents = loader.load()\n",
        "#     # split the documents into chunks\n",
        "#     text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "#     texts = text_splitter.split_documents(documents)\n",
        "#     # select which embeddings we want to use\n",
        "#     embeddings = OpenAIEmbeddings()\n",
        "#     # create the vectorestore to use as the index\n",
        "#     db = Chroma.from_documents(texts, embeddings)\n",
        "#     # expose this index in a retriever interface\n",
        "#     retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
        "#     # create a chain to answer questions \n",
        "#     qa = RetrievalQA.from_chain_type(\n",
        "#         llm=OpenAI(), chain_type=chain_type, retriever=retriever, return_source_documents=True)\n",
        "#     result = qa({\"query\": query})\n",
        "#     print(result['result'])\n",
        "#     return result"
      ],
      "metadata": {
        "id": "thZlkzUysHn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the vectorestore to use as the index\n",
        "db = Chroma.from_documents(texts, embeddings)\n",
        "# expose this index in a retriever interface\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 8})"
      ],
      "metadata": {
        "id": "0ATnPHXIE5fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a chain to answer questions \n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\", retriever=retriever, return_source_documents=False)\n",
        "result = qa({\"query\": query})\n",
        "print(result['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLaENM20FHIk",
        "outputId": "64c31ac8-99a9-44cf-c13c-7eb5fe04ff6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The resumes with statistics skills are: \n",
            "\n",
            "Data Analysis Intern (page 1, source file 1), \n",
            "Data Science Leadership and Activities (page 5, source file 1), \n",
            "Covid19 Data Visualization in Python (page 7, source file 1), \n",
            "Huijin Asset Management (page 9, source file 1).\n",
            "\n",
            "The JSON format would look like this: \n",
            "[\n",
            "  {\n",
            "    \"page_id\": 1,\n",
            "    \"source_file\": 1\n",
            "  },\n",
            "  {\n",
            "    \"page_id\": 5,\n",
            "    \"source_file\": 1\n",
            "  },\n",
            "  {\n",
            "    \"page_id\": 7,\n",
            "    \"source_file\": 1\n",
            "  },\n",
            "  {\n",
            "    \"page_id\": 9,\n",
            "    \"source_file\": 1\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "rgTmCwGbAwqf",
        "outputId": "d67af98a-52f2-4daa-b3f5-f2d0f72f234b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'which resumes have statistics skills? Provide resulting resumes in JSON format with the following keys from the document metadata fields: page_id, source_file'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a chain to answer questions \n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\", retriever=retriever, return_source_documents=False)\n",
        "result = qa({\"query\": query})\n",
        "print(result['result'])"
      ],
      "metadata": {
        "id": "jPg9eATEIr07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25fdfafb-e0a6-470d-90a8-07c6b25218b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The resumes with statistics skills are the resumes with the page_ids of 1, 4, and 5. The JSON format for the resulting resumes is: \n",
            "[\n",
            "  {\n",
            "    \"page_id\": 1,\n",
            "    \"source_file\": \"resume1.pdf\"\n",
            "  },\n",
            "  {\n",
            "    \"page_id\": 4,\n",
            "    \"source_file\": \"resume2.pdf\"\n",
            "  },\n",
            "  {\n",
            "    \"page_id\": 5,\n",
            "    \"source_file\": \"resume3.pdf\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try prompt templates?"
      ],
      "metadata": {
        "id": "7gVEwF3Y-xSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YItbaF6kF1Zf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}